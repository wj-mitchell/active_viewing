---
output:
  pdf_document: default
  html_document: default
---
# README
  ***v2023.09.28***

## Warning: 
This data must be secured at all times. Do not share it with unauthorized individuals or move it to any locations or devices beyond the directory it is currently contained within. Ask [Billy Mitchell](mailto:billy.mitchell@temple.edu) if you have any questions about this. 

## Background
This directory has been built for Caroline George's project and contains some of the materials necessary to annotate stimuli, create an annotation coding manual, transcribe free response data, and categorize the free-response data generated during the Uncertainty Study using these annotations. These materials are copies created to make collaboration simple since only a limited group of people have access to the shared drive where the originals are stored. Anything generated from this collaboration should be transferred to the original directory at `/Helion_Group/studies/uncertainty/studies_neuro/data`. This directory should contain the following files and/or directories:

* **Video Stimuli** - The stimuli (Episode 4 of *HBO*'s limited series *The Undoing*) that participants watch during the task in .mp4 format split into two equal-length videos (22m 17s). These have been processed to improve playback and sound quality during scanning. The filenames are `StimVidTestFirstHalf_comp.mp4` and `StimVidTestLastHalf_comp.mp4` and may be compressed to a .zip format to make sharing easier. These are contained within the **Stimuli** directory.
* **Background Information** - An informational primer generated in .pdf format to help our previous annotation person understand the basic details of the story. Could also be helpful for a first time watcher. This contains more information that participants are given, though they could likely piece together much of this information on their own as they watch the narrative unfold. The filename is `Background.pdf`. This is also contained within the **Stimuli** directory.
* **Transcriptions & Recordings** - The de-identified recordings participants generated as they were completing the study, as well as the transcriptions generated by speech-to-text services like *Whisper* and *Otter* and cleaned by Research Assistants. Transcriptions generated by each of these services are contained within respectively named folders. The participant recordings are contained within a directory titled *Recordings*. The RA reviewed and approved transcriptions are contained within a directory titled *Final*. Additional files include a `.md` and `.pdf` version of the detailed guide given to transcribers (`Transcription_Guide`) as well as a QA and progress tracking sheet (`Transcription_Tracker.xlsx`).
* **Scripts** - Any scripts generated for the purpose of analyzing or generating data are contained within this aptly named directory. 
* **Detailed Annotations** - A previous attempt to annotate this video was made, but may have been inappropriate for the needs of what we're trying to do. These may be helpful when we get to redoing detailed annotations because we can just modify what we already have rather than starting from scratch, but probably contain more information than is helpful if we're just trying to capture scene order. They are in .csv format and are named `Annotations_FirstHalf.csv` and `Annotations_LastHalf.csv`. These are contained within the *Annotations* directory.
* **Scene Annotations** - Caroline generated a broad annotation capturing the time at which each scene starts, a thorough description of what occurs within each scence, and an accounting of when a given scene ends. This file is named `StimVid-Timestamps.xlsx` and is contained within the *Annotations* directory.
* **Independent Rater Coding Manual** - Caroline has also generated an early version of the coding manual we will likely use to train coders for this project. This is titled `Certainty Predicting Memory Coding Manual.docx`. Additionally, an example of a coding manual that Billy had generated for his previous haunted house study is included in which coders were tasked with identifying which regulation strategies people were deploying based upon their free responses. This was included to help Caroline get an idea of the coding manual we need so that we can train two independent coders (likely undergraduate RAs) to review the free-recall texts and categorize components of them. For example, we might want undergraduates to sort free-recall text based upon which scene they believe the text is describing so that we can then look at the order in which the participant recalled the scenes. We ideally want to generate a written guide containing information that would explain what we want the RAs to do and how they should do it in a systematic, relatively-objective way. The two documents included are a classification manual (`Classif_Manual_Example.pdf`) and a training exercise (`Classif_Exercise_Example.docx`). **Doing some sort of a practice or training exercise before diving into the actual coding is strongly recommended**. Below I also include the description of the independent coders process that I used during this project:

        During piloting, two hypotheses-blind raters classified strategy descriptions into one or more strategy categories: Reappraisal, Distraction,Suppression, Situation Selection, Situation Modification, or 'None of the above'(IRR Agreement = 0.880).Raters were also provided the participant's description of the event andthe emotions they experienced which they indicated having downregulated. Raters were undergraduate research assistants who trained by first reviewing examples of landmark literature which defined the strategies of interest as commonly used in the field (Gross, 1998, 2002). Raters then reviewed select methodological excerpts from experimental papers to see how cognitive reappraisal, attention deployment, and other Process Model strategies were defined within past studies(Shafir et al., 2016; Sheppes et al., 2011). Lastly, raters independently completed classification exercises using examples of regulation strategy descriptions from the same context but which were not included in the primary dataset. Through the training and classification process, raters were instructed not to collaborate or discuss their ratings with each other during the rating process. After individually classifying each description, raters emailed their assessments to the moderator, who compared the ratings for disagreements (i.e., cases in which raters disagreed on how a regulation event should be classified). The moderator then met with both raters remotely using a digital video conferencing platform and moderated a review of the classifications, asking raters to compromise in cases of classification disagreement. The moderator's role was to facilitate discussion of classifications and documenttheir conclusions, but was not involved in the discussion and disconnected during them (i.e., muted their microphone; turned off camera)to avoid unduly influencing the outcome ... Two hypotheses-blind raters classified each observation's strategy description in Study 1 into one or more strategy categories: Reappraisal, Distraction, Suppression, a combination of the three, or none of the above (IRR Agreement = 0.877)... Raters were undergraduate research assistants who were trained using the same methodology described in the pilot study, but were not the same raters from the pilot study...

## Plans, Goals, & Timeline
Our current plan is as follows:
1. To have an undergraduate RA, potentially Jackson, review all of our free recall recordings transcriptions. Many of these had already been processed by Otter.AI, an automated speech-to-text transcription service, and fixed / reviewed by Brett Kunkel. However, we are no longer using Otter.AI as a lab and have since moved onto Whisper, which is another speech-to-text service that has been privacy for our data and is better at transcribing voices with background noise. Some free-response recordings never got transcribed with Otter and never got reviewed. Jackson will go through all transcriptions, double checking Brett's work and using the new transcriptions generated by Whisper to fill in the blanks that Brett was not able to. Jackson will also give a first pass to the recordings that we have not yet tackled. Lastly, he will identify any recordings that we may not have adequate signal-to-noise unpon and which may be lost data; 
    
    **ESTIMATED TIME: 2 weeks**  
    **STATUS: In Progress**

2. Generate annotations of the stimulus to function as an objective metric by which to compare free recall. This can occur simultaneously with the first step. Caroline will take care of this. We specifically want an accounting of the time at which each scene starts, a thorough description of what occurs within each scence, and an accounting of when a given scene ends. 

    **ESTIMATED TIME: 1 week**  
    **STATUS: Completed; Needs Review**

3. Generate or improve the detailed annotations of the stimulus. How detailed that we need to be is still a little up in the air. One system that we talked about using was the Autobiographical Inventory, or AI. Alternatively, we could use the systems laid out by Andrew Heusser and Janice Chen in their papers. 
    * The AI System is a means of documenting the content of memory recall from lived experiences. It breaks details down into categories, such as Event, Place, Time, Perception, Semantic, Emotion/Thought, and Other details, with an additional category for repetitions as well. While very extensive, this may be overkill relative to our needs. Regardless, I did acquire training materials which I stored in the main directory under a subdirectory titled `AI`.  
    * Janice Chen (and Andrew Heusser by association) broke their 50 minute stimulus into 1000 segments of meaningful content by a hypothesis-blind human rater (mean duration 3.0 seconds, s.d. 2.2 seconds) who then labelled the content according to: 
        + (a.) How many people are present onscreen; 
        + (b.) What specific location is being shown; 
        + (c.) Whether the location is indoor or outdoor; 
        + (d.) Whether or not anyone is speaking; 
        + (e.) Excitement/engagement/activity level; 
        + (f.) Positive or negative mood; 
        + (g.) Whether or not there is music playing; 
        + (h.) Whether or not there are written words onscreen; 
        + (i.) Whether or not the certain characters are onscreen. 
        
        For Arousal and Valence, assessments were collected from four different raters (Arousal: Cronbach's alpha = 0.75; Valence: Cronbach's alpha = 0.81) and the average across raters used for model prediction. The other eight labels were deemed objective and not requiring multiple raters
    * We currently have the follow variables captured: 
        + (a.) Narrative Descriptions; 
        + (b.) What specific location is being shown; 
        + (c.) Whether the location is indoor or outdoor; 
        + (d.) Whether a scene change has occurred; 
        + (e.) Whether or not anyone is speaking; 
        + (f.) Whether or not there is music playing; 
        + (g.) Whether or not there are written words onscreen; 
        + (h.) Whether or not the certain characters are onscreen; and 
        + (i.) Which characters are in focus. 
        
        Variables not captured include: 
        + (a.) How many people *in general* are present onscreen; 
        + (b.) Excitement/engagement/activity level; 
        + (c.) Positive or negative mood; 
        
        though, the latter two could probably be supplemented with the use of *EmoNet* or *Hume*. 

    **ESTIMATED TIME: 2 weeks**  
    **STATUS: In Consideration**

4. Generate a coding or classification manual to train independent coders who will go through transcriptions and classify the text based upon the annotations. Features of the data generated in this step (i.e., continuity, accuracy, granularity) will be the outcome or critieria variables that we use in fututre analyses. We would want to be very thorough in generating this manual, get feedback on it and test the manual with some "pilot participants" because if your coders aren't on the same page when they start it can cause massive headaches for you later.

    **ESTIMATED TIME: 2 weeks**  
    **STATUS: In Consideration**

5. Actually code the free recall responses according to the schema developed. Independent coders should do this on their own, but setting explicit goals (i.e., how many transcriptions that they should complete within a given time) is very important and should be checked in on regularly. This could be time consuming. At the end of the period, the coders and the person overseeing them should meet and review their results, finding points in which they agree and coming to a compromise at points in which they don't agree. Billy could advise more at this stage.

    **ESTIMATED TIME: 2 weeks**  
    **STATUS: Pending**

6. At this stage, we should have all of the data that we care about and it then simple becomes an issue of wrangling that data into a format that is usable and suited for analyses. The behavioral uncertainty rating data and individual difference measures have already been processed and are ready for use in an analysis. This will likely occur in R. 

    **ESTIMATED TIME: 2 weeks**  
    **STATUS: Pending**    

## Contact:
If you have any questions or concerns, please contact Billy Mitchell at billy.mitchell@temple.edu